import gradio as gr
import subprocess
import os
import time
import threading
from gemini_chat import GeminiChatSession
from system_prompt import prompt

class VirtualChatApp:
    def __init__(self):
        self.chat_session = GeminiChatSession(
            model="gemini-2.0-flash",
            system_instruction=prompt
        )
        self.chat_history = []
        
    def process_message(self, user_input, history):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯çš„å®Œæ•´æµç¨‹"""
        if not user_input.strip():
            return history, "", None
            
        # 1. è°ƒç”¨Geminiç”Ÿæˆå›å¤æ–‡æœ¬
        print(f"ç”¨æˆ·è¾“å…¥: {user_input}")
        ai_response = self.chat_session.send_message(user_input)
        
        if not ai_response:
            ai_response = "Sorry, I couldn't generate a response. Please try again."
        
        print(f"AIå›å¤: {ai_response}")
        
        # 2. ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        audio_filename = f"audio_{int(time.time())}.wav"
        audio_cmd = f"python generate_audio.py \"{ai_response}\" {audio_filename}"
        
        print(f"æ‰§è¡ŒéŸ³é¢‘ç”Ÿæˆå‘½ä»¤: {audio_cmd}")
        try:
            result = subprocess.run(audio_cmd, shell=True, capture_output=True, text=True, timeout=60)
            if result.returncode != 0:
                print(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return history + [[user_input, ai_response]], ai_response, None
        except subprocess.TimeoutExpired:
            print("éŸ³é¢‘ç”Ÿæˆè¶…æ—¶")
            return history + [[user_input, ai_response]], ai_response, None
        
        # 3. ç”Ÿæˆè§†é¢‘æ–‡ä»¶
        video_filename = f"video_{int(time.time())}.mp4"
        base_video_path = "/kaggle/input/audio-demo1/kling_20250806_Image_to_Video_this_woman_3742_0.mp4"
        video_cmd = f"python generate_video.py \"{base_video_path}\" {audio_filename} {video_filename}"
        
        print(f"æ‰§è¡Œè§†é¢‘ç”Ÿæˆå‘½ä»¤: {video_cmd}")
        try:
            result = subprocess.run(video_cmd, shell=True, capture_output=True, text=True, timeout=120)
            if result.returncode != 0:
                print(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return history + [[user_input, ai_response]], ai_response, None
        except subprocess.TimeoutExpired:
            print("è§†é¢‘ç”Ÿæˆè¶…æ—¶")
            return history + [[user_input, ai_response]], ai_response, None
        
        # 4. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(video_filename):
            print(f"è§†é¢‘ç”ŸæˆæˆåŠŸ: {video_filename}")
            return history + [[user_input, ai_response]], ai_response, video_filename
        else:
            print("è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°")
            return history + [[user_input, ai_response]], ai_response, None
    
    def clear_chat(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        self.chat_session.clear_history()
        return [], "", None

def create_interface():
    app = VirtualChatApp()
    
    with gr.Blocks(title="Virtual Chat", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¤– Virtual Chat with AI")
        gr.Markdown("Chat with an AI virtual host! The AI will respond with both text and video.")
        
        with gr.Row():
            with gr.Column(scale=2):
                # èŠå¤©å†å²æ˜¾ç¤º
                chatbot = gr.Chatbot(
                    label="Chat History",
                    height=400,
                    show_label=True
                )
                
                # ç”¨æˆ·è¾“å…¥æ¡†
                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="Type your message here...",
                        label="Your Message",
                        lines=2,
                        scale=4
                    )
                    send_btn = gr.Button("Send", variant="primary", scale=1)
                
                # æ§åˆ¶æŒ‰é’®
                with gr.Row():
                    clear_btn = gr.Button("Clear Chat", variant="secondary")
            
            with gr.Column(scale=1):
                # è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
                video_output = gr.Video(
                    label="AI Response Video",
                    height=400,
                    autoplay=True
                )
                
                # çŠ¶æ€æ˜¾ç¤º
                status_text = gr.Textbox(
                    label="Status",
                    value="Ready to chat!",
                    interactive=False,
                    lines=3
                )
        
        # äº‹ä»¶å¤„ç†
        def handle_send(user_input, history):
            if not user_input.strip():
                return history, "", None, "Please enter a message."
            
            new_history, ai_response, video_file = app.process_message(user_input, history)
            
            if video_file:
                status_msg = f"âœ… Generated video: {video_file}"
            else:
                status_msg = "âš ï¸ Video generation failed, but text response is ready."
            
            return new_history, "", video_file, status_msg
        
        def handle_clear():
            return app.clear_chat()
        
        # ç»‘å®šäº‹ä»¶
        send_btn.click(
            handle_send,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, msg_input, video_output, status_text]
        )
        
        msg_input.submit(
            handle_send,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, msg_input, video_output, status_text]
        )
        
        clear_btn.click(
            handle_clear,
            outputs=[chatbot, msg_input, video_output]
        )
        
        # ç¤ºä¾‹
        gr.Markdown("""
        ### ğŸ’¡ Tips:
        - Type your message and press Enter or click Send
        - The AI will respond in English with both text and video
        - Videos are generated using your specified base video and AI-generated audio
        - Use "Clear Chat" to reset the conversation
        """)
    
    return demo

if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        debug=True
    )
